{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca041c69",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\">ARTIFICIAL NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0475e9",
   "metadata": {},
   "source": [
    "# Task1- Data Exploration and Preprocessing\n",
    "- Loading and exploring the dataset\n",
    "- Summarizing key features\n",
    "- Data preprocessing (normalization, handling missing values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694c03a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580941ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter</th>\n",
       "      <th>xbox</th>\n",
       "      <th>ybox</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>onpix</th>\n",
       "      <th>xbar</th>\n",
       "      <th>ybar</th>\n",
       "      <th>x2bar</th>\n",
       "      <th>y2bar</th>\n",
       "      <th>xybar</th>\n",
       "      <th>x2ybar</th>\n",
       "      <th>xy2bar</th>\n",
       "      <th>xedge</th>\n",
       "      <th>xedgey</th>\n",
       "      <th>yedge</th>\n",
       "      <th>yedgex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>C</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>T</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      letter  xbox  ybox  width  height  onpix  xbar  ybar  x2bar  y2bar  \\\n",
       "0          T     2     8      3       5      1     8    13      0      6   \n",
       "1          I     5    12      3       7      2    10     5      5      4   \n",
       "2          D     4    11      6       8      6    10     6      2      6   \n",
       "3          N     7    11      6       6      3     5     9      4      6   \n",
       "4          G     2     1      3       1      1     8     6      6      6   \n",
       "...      ...   ...   ...    ...     ...    ...   ...   ...    ...    ...   \n",
       "19995      D     2     2      3       3      2     7     7      7      6   \n",
       "19996      C     7    10      8       8      4     4     8      6      9   \n",
       "19997      T     6     9      6       7      5     6    11      3      7   \n",
       "19998      S     2     3      4       2      1     8     7      2      6   \n",
       "19999      A     4     9      6       6      2     9     5      3      1   \n",
       "\n",
       "       xybar  x2ybar  xy2bar  xedge  xedgey  yedge  yedgex  \n",
       "0          6      10       8      0       8      0       8  \n",
       "1         13       3       9      2       8      4      10  \n",
       "2         10       3       7      3       7      3       9  \n",
       "3          4       4      10      6      10      2       8  \n",
       "4          6       5       9      1       7      5      10  \n",
       "...      ...     ...     ...    ...     ...    ...     ...  \n",
       "19995      6       6       4      2       8      3       7  \n",
       "19996     12       9      13      2       9      3       7  \n",
       "19997     11       9       5      2      12      2       4  \n",
       "19998     10       6       8      1       9      5       8  \n",
       "19999      8       1       8      2       7      2       8  \n",
       "\n",
       "[20000 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabets = pd.read_csv(r'C:\\Users\\Aabshaar\\Downloads\\Neural networks\\Neural networks\\Alphabets_data.csv')\n",
    "alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d98df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2866de33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   letter  20000 non-null  object\n",
      " 1   xbox    20000 non-null  int64 \n",
      " 2   ybox    20000 non-null  int64 \n",
      " 3   width   20000 non-null  int64 \n",
      " 4   height  20000 non-null  int64 \n",
      " 5   onpix   20000 non-null  int64 \n",
      " 6   xbar    20000 non-null  int64 \n",
      " 7   ybar    20000 non-null  int64 \n",
      " 8   x2bar   20000 non-null  int64 \n",
      " 9   y2bar   20000 non-null  int64 \n",
      " 10  xybar   20000 non-null  int64 \n",
      " 11  x2ybar  20000 non-null  int64 \n",
      " 12  xy2bar  20000 non-null  int64 \n",
      " 13  xedge   20000 non-null  int64 \n",
      " 14  xedgey  20000 non-null  int64 \n",
      " 15  yedge   20000 non-null  int64 \n",
      " 16  yedgex  20000 non-null  int64 \n",
      "dtypes: int64(16), object(1)\n",
      "memory usage: 2.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Displaying basic information about the given dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(alphabets.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce91ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the dataset:\n",
      "  letter  xbox  ybox  width  height  onpix  xbar  ybar  x2bar  y2bar  xybar  \\\n",
      "0      T     2     8      3       5      1     8    13      0      6      6   \n",
      "1      I     5    12      3       7      2    10     5      5      4     13   \n",
      "2      D     4    11      6       8      6    10     6      2      6     10   \n",
      "3      N     7    11      6       6      3     5     9      4      6      4   \n",
      "4      G     2     1      3       1      1     8     6      6      6      6   \n",
      "\n",
      "   x2ybar  xy2bar  xedge  xedgey  yedge  yedgex  \n",
      "0      10       8      0       8      0       8  \n",
      "1       3       9      2       8      4      10  \n",
      "2       3       7      3       7      3       9  \n",
      "3       4      10      6      10      2       8  \n",
      "4       5       9      1       7      5      10  \n"
     ]
    }
   ],
   "source": [
    "# Displaying the first few rows of the dataset\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(alphabets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef79b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in the dataset:\n",
      "letter    0\n",
      "xbox      0\n",
      "ybox      0\n",
      "width     0\n",
      "height    0\n",
      "onpix     0\n",
      "xbar      0\n",
      "ybar      0\n",
      "x2bar     0\n",
      "y2bar     0\n",
      "xybar     0\n",
      "x2ybar    0\n",
      "xy2bar    0\n",
      "xedge     0\n",
      "xedgey    0\n",
      "yedge     0\n",
      "yedgex    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "print(alphabets.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we don't have nulls in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29710865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ab942b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['letter', 'xbox', 'ybox', 'width', 'height', 'onpix', 'xbar', 'ybar',\n",
       "       'x2bar', 'y2bar', 'xybar', 'x2ybar', 'xy2bar', 'xedge', 'xedgey',\n",
       "       'yedge', 'yedgex'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding Target Var positing for normalizing the data\n",
    "alphabets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b9d9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = alphabets.drop(columns=['letter']) \n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2516afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the target column back to the normalized features\n",
    "processed_data = pd.DataFrame(normalized_features, columns=features.columns)\n",
    "processed_data['letter'] = alphabets['letter']  # Replace 'target' with the actual target column name\n",
    "\n",
    "# Saving the processed data to a new CSV file\n",
    "processed_data.to_csv(\"Processed_Alphabets_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27de8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46b72962",
   "metadata": {},
   "source": [
    "Summarize its key features such as the number of samples, features, and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff7377aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of samples: 20000\n",
      "Number of features: 16\n"
     ]
    }
   ],
   "source": [
    "# Number of samples (rows) and features (columns)\n",
    "num_samples = alphabets.shape[0]\n",
    "num_features = alphabets.shape[1] - 1  # excluding the target column\n",
    "\n",
    "print(\"\\nNumber of samples:\", num_samples)\n",
    "print(\"Number of features:\", num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87b76414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique classes: 26\n",
      "Classes: ['T' 'I' 'D' 'N' 'G' 'S' 'B' 'A' 'J' 'M' 'X' 'O' 'R' 'F' 'C' 'H' 'W' 'L'\n",
      " 'P' 'E' 'V' 'Y' 'Q' 'U' 'K' 'Z']\n"
     ]
    }
   ],
   "source": [
    "# Identify the unique classes in the target column\n",
    "unique_classes = alphabets['letter'].unique()\n",
    "num_classes = len(unique_classes)\n",
    "\n",
    "print(\"\\nNumber of unique classes:\", num_classes)\n",
    "print(\"Classes:\", unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c92651e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics of the dataset:\n",
      "       letter          xbox          ybox         width       height  \\\n",
      "count   20000  20000.000000  20000.000000  20000.000000  20000.00000   \n",
      "unique     26           NaN           NaN           NaN          NaN   \n",
      "top         U           NaN           NaN           NaN          NaN   \n",
      "freq      813           NaN           NaN           NaN          NaN   \n",
      "mean      NaN      4.023550      7.035500      5.121850      5.37245   \n",
      "std       NaN      1.913212      3.304555      2.014573      2.26139   \n",
      "min       NaN      0.000000      0.000000      0.000000      0.00000   \n",
      "25%       NaN      3.000000      5.000000      4.000000      4.00000   \n",
      "50%       NaN      4.000000      7.000000      5.000000      6.00000   \n",
      "75%       NaN      5.000000      9.000000      6.000000      7.00000   \n",
      "max       NaN     15.000000     15.000000     15.000000     15.00000   \n",
      "\n",
      "               onpix          xbar          ybar         x2bar         y2bar  \\\n",
      "count   20000.000000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean        3.505850      6.897600      7.500450      4.628600      5.178650   \n",
      "std         2.190458      2.026035      2.325354      2.699968      2.380823   \n",
      "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%         2.000000      6.000000      6.000000      3.000000      4.000000   \n",
      "50%         3.000000      7.000000      7.000000      4.000000      5.000000   \n",
      "75%         5.000000      8.000000      9.000000      6.000000      7.000000   \n",
      "max        15.000000     15.000000     15.000000     15.000000     15.000000   \n",
      "\n",
      "               xybar       x2ybar        xy2bar         xedge        xedgey  \\\n",
      "count   20000.000000  20000.00000  20000.000000  20000.000000  20000.000000   \n",
      "unique           NaN          NaN           NaN           NaN           NaN   \n",
      "top              NaN          NaN           NaN           NaN           NaN   \n",
      "freq             NaN          NaN           NaN           NaN           NaN   \n",
      "mean        8.282050      6.45400      7.929000      3.046100      8.338850   \n",
      "std         2.488475      2.63107      2.080619      2.332541      1.546722   \n",
      "min         0.000000      0.00000      0.000000      0.000000      0.000000   \n",
      "25%         7.000000      5.00000      7.000000      1.000000      8.000000   \n",
      "50%         8.000000      6.00000      8.000000      3.000000      8.000000   \n",
      "75%        10.000000      8.00000      9.000000      4.000000      9.000000   \n",
      "max        15.000000     15.00000     15.000000     15.000000     15.000000   \n",
      "\n",
      "               yedge       yedgex  \n",
      "count   20000.000000  20000.00000  \n",
      "unique           NaN          NaN  \n",
      "top              NaN          NaN  \n",
      "freq             NaN          NaN  \n",
      "mean        3.691750      7.80120  \n",
      "std         2.567073      1.61747  \n",
      "min         0.000000      0.00000  \n",
      "25%         2.000000      7.00000  \n",
      "50%         3.000000      8.00000  \n",
      "75%         5.000000      9.00000  \n",
      "max        15.000000     15.00000  \n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nSummary statistics of the dataset:\")\n",
    "print(alphabets.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b471454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dc4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541a261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23705f83",
   "metadata": {},
   "source": [
    "# Task 2: Model Implementation\n",
    "- Construct a basic ANN model\n",
    "- Divide the dataset into training and test sets\n",
    "- Train the model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a17e93ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15844\\451599177.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Spliting the data into features and target\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'letter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'letter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;36m0.\u001b[0m \u001b[1;36m0.\u001b[0m \u001b[1;36m0.\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"int\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    870\u001b[0m               dtype='datetime64[ns]')\n\u001b[0;32m    871\u001b[0m         \"\"\"\n\u001b[1;32m--> 872\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'T'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Loading the new processed dataset, which we made\n",
    "data = pd.read_csv(\"Processed_Alphabets_data.csv\")\n",
    "\n",
    "# Spliting the data into features and target\n",
    "x = data.drop(columns=['letter'])\n",
    "y = to_categorical(data['letter'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fddd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target var is in obj so we are getting this error.  Let's convert it into int using LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b23d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Loading the new processed dataset, which we made\n",
    "data = pd.read_csv(\"Processed_Alphabets_data.csv\")\n",
    "\n",
    "# Splitting the data into features and target\n",
    "x = data.drop(columns=['letter'])\n",
    "\n",
    "# Encoding string labels to integers\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(data['letter'])\n",
    "\n",
    "# Converting integer labels to categorical format\n",
    "y = to_categorical(y_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac640c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5fc08a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "befba96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))  # Output layer with softmax activation for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21826b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48f6267a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 6s 7ms/step - loss: 1.7912 - accuracy: 0.5216\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.8924 - accuracy: 0.7499\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.6988 - accuracy: 0.8023\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.5871 - accuracy: 0.8305\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.5090 - accuracy: 0.8512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa10417520>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b30f8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1s 3ms/step - loss: 0.4760 - accuracy: 0.8597\n",
      "Test accuracy = 0.859749972820282\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "dloss, daccuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy =\", daccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97976062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49ef4ef7",
   "metadata": {},
   "source": [
    "# Task 3: Hyperparameter Tuning\n",
    "- Modify hyperparameters\n",
    "- Use grid search or random search for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to wrap our Keras model in a KerasClassifier and then define a function to build model. \n",
    "# Since we have larger dataset, we'll use Random Seacrh instead of Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "657b6653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 16)\n",
      "(4000, 16)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3723779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aabshaar\\AppData\\Local\\Temp\\ipykernel_15844\\2830712913.py:28: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=build_model, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'optimizer': 'adam', 'neurons': 128, 'learning_rate': 0.001, 'hidden_layers': 3, 'epochs': 10, 'batch_size': 20, 'activation': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define a function to build the model\n",
    "def build_model(optimizer='adam', neurons=32, activation='relu', hidden_layers=1, learning_rate=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=16, activation=activation))  # Adjust input_dim as per your data\n",
    "    \n",
    "    for _ in range(hidden_layers):\n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "    \n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Adjust for your task\n",
    "    return model\n",
    "\n",
    "# Wrap the model using KerasClassifier\n",
    "model = KerasClassifier(build_fn=build_model, verbose=0)\n",
    "\n",
    "# Define the parameter grid\n",
    "search_dict = {\n",
    "    'optimizer': ['adam', 'SGD'],\n",
    "    'neurons': [32, 64, 128],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'batch_size': [10, 20],\n",
    "    'epochs': [10, 50],\n",
    "    'hidden_layers': [1, 2, 3],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=search_dict, n_iter=10, cv=3, random_state=42, n_jobs=-1)\n",
    "random_search_result = random_search.fit(x_train, y_train)\n",
    "\n",
    "# Display the best parameters\n",
    "print(\"Best parameters are:\", random_search_result.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea577f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fd167c5",
   "metadata": {},
   "source": [
    "## Re-building the model using the Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9df6045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "800/800 [==============================] - 4s 4ms/step - loss: 0.0879 - accuracy: 0.6054\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.0335 - accuracy: 0.8486\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.0225 - accuracy: 0.9022\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.0172 - accuracy: 0.9270\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.0141 - accuracy: 0.9411\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.0120 - accuracy: 0.9521\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.0102 - accuracy: 0.9582\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.0090 - accuracy: 0.9626\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.0082 - accuracy: 0.9673\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.0072 - accuracy: 0.9720\n"
     ]
    }
   ],
   "source": [
    "# Define the model with 3 hidden layers\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(128, input_dim=x_train.shape[1], activation='relu'))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # For binary classification\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2a97cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1s 3ms/step - loss: 0.0114 - accuracy: 0.9507\n",
      "Test accuracy = 0.9507499933242798\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "new_loss, new_accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy =\", new_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0fff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152dc145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5fc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f7121db",
   "metadata": {},
   "source": [
    "# 4. Evaluation\n",
    "- Employ suitable metrics such as accuracy, precision, recall, and F1-score to evaluate your model's performance.\n",
    "- Discuss the performance differences between the model with default hyperparameters and the tuned model, emphasizing the effects of hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d56f342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       149\n",
      "           1       0.97      0.92      0.94       153\n",
      "           2       0.95      0.93      0.94       137\n",
      "           3       0.98      0.88      0.93       156\n",
      "           4       0.94      0.95      0.95       141\n",
      "           5       0.98      0.89      0.93       140\n",
      "           6       0.94      0.93      0.93       160\n",
      "           7       0.90      0.93      0.91       144\n",
      "           8       0.96      0.96      0.96       146\n",
      "           9       0.97      0.95      0.96       149\n",
      "          10       0.96      0.84      0.90       130\n",
      "          11       0.99      0.97      0.98       155\n",
      "          12       0.98      0.98      0.98       168\n",
      "          13       1.00      0.91      0.96       151\n",
      "          14       0.89      0.92      0.90       145\n",
      "          15       0.96      0.98      0.97       173\n",
      "          16       0.90      0.99      0.95       166\n",
      "          17       0.86      0.97      0.91       160\n",
      "          18       0.98      0.97      0.97       171\n",
      "          19       0.97      0.98      0.97       163\n",
      "          20       0.98      0.97      0.97       183\n",
      "          21       0.97      0.94      0.95       158\n",
      "          22       0.99      0.95      0.97       148\n",
      "          23       0.96      1.00      0.98       154\n",
      "          24       1.00      0.96      0.98       168\n",
      "          25       0.96      0.98      0.97       132\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      4000\n",
      "   macro avg       0.96      0.95      0.95      4000\n",
      "weighted avg       0.96      0.95      0.95      4000\n",
      " samples avg       0.95      0.95      0.95      4000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aabshaar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)  # Converting probabilities to binary predictions\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef14ecd",
   "metadata": {},
   "source": [
    "# Output Explanation\n",
    "\n",
    "Precision: The proportion of positive identifications that were actually correct.\n",
    "\n",
    "Recall: The proportion of actual positives that were correctly identified.\n",
    "\n",
    "F1-Score: The harmonic mean of precision and recall, providing a single metric to evaluate the model’s performance.\n",
    "\n",
    "Support: The number of true instances for each class\n",
    "\n",
    "Micro Average: Calculates metrics globally by counting the total true positives, false negatives, and false positives. It is useful when you want to see the overall performance of the model across all classes.\n",
    "\n",
    "Macro Average: Calculates metrics for each class independently and then takes the average. It treats all classes equally, regardless of their support.\n",
    "\n",
    "Weighted Average: Calculates metrics for each class and then takes the average, weighted by the number of true instances for each class. It accounts for class imbalance.\n",
    "\n",
    "Samples Average: This is used for multi-label classification problems. It calculates metrics for each label and then averages them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862eef6f",
   "metadata": {},
   "source": [
    "# Observations:\n",
    "\n",
    "### High Precision, Recall, and F1-Scores: \n",
    "- It indicates that the model seems to be performing well.\n",
    "\n",
    "### micro avg, macro avg, and weighted avg values:\n",
    "- They are close, indicating balanced performance across classes and suggesting that the model is generally effective.\n",
    "\n",
    "### Sample Size: \n",
    "- The support values show the number of samples for each class, which can help understand if certain classes are underrepresented.\n",
    "\n",
    "    - Overall, this report indicates that the tuned model is performing well across multiple metrics and classes. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51ec6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22b7f92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Performance Comparison--\n",
      "Model with Default Hyperparameters(Test Accuracy) =  0.8640000224113464\n",
      "Model with Default Hyperparameters(Loss)          =  0.4667240083217621\n",
      "------------------------------------------------------------------------------\n",
      "Model with Tuned Hyperparameters(Test Accuracy) =  0.9507499933242798\n",
      "Model with Tuned Hyperparameters(Loss)          =  0.011359731666743755\n"
     ]
    }
   ],
   "source": [
    "print(\"--Performance Comparison--\")\n",
    "\n",
    "print(\"Model with Default Hyperparameters(Test Accuracy) = \", daccuracy)\n",
    "print(\"Model with Default Hyperparameters(Loss)          = \", dloss)\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"Model with Tuned Hyperparameters(Test Accuracy) = \", new_accuracy)\n",
    "print(\"Model with Tuned Hyperparameters(Loss)          = \", new_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625af480",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Analysis of Hyperparameter Tuning Effects:\n",
    "\n",
    "## Accuracy Improvement:\n",
    "Default Model: 86%\n",
    "\n",
    "Tuned Model: 95%\n",
    "\n",
    "Conclusion: The accuracy increased by approximately 9% after tuning. This suggests that the tuned model performs significantly better on unseen data, indicating that the tuning process helped in optimizing the model to generalize better.\n",
    "\n",
    "\n",
    "## Loss Reduction:\n",
    "\n",
    "Default Model: 0.4667\n",
    "\n",
    "Tuned Model: 0.0113\n",
    "\n",
    "Conclusion: The loss decreased drastically by 0.4554. A lower loss value indicates that the tuned model makes fewer errors and performs better in predicting the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5426cc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b78ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19499f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "394a9f63",
   "metadata": {},
   "source": [
    "# Last Task : Evaluation Criteria\n",
    "●\tAccuracy and completeness of the implementation.\n",
    "\n",
    "●\tProficiency in data preprocessing and model development.\n",
    "\n",
    "●\tSystematic approach and thoroughness in hyperparameter tuning.\n",
    "\n",
    "●\tDepth of evaluation and discussion.\n",
    "\n",
    "●\tOverall quality of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f27e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a029bab",
   "metadata": {},
   "source": [
    "# Evaluation of the ANN Model Development and Tuning Project\n",
    "\n",
    "## 1. Accuracy and Completeness of the Implementation\n",
    "### Accuracy:\n",
    "- The ANN model has achieved an impressive accuracy of over 95% on the test set after performing HyperParameter Tuning. - This high accuracy indicates that the model is effectively distinguishing between different classes in the dataset. \n",
    "- It also includes key preprocessing steps, such as normalizing features and encoding categorical variables.\n",
    "\n",
    "### Completeness:\n",
    "- All necessary preprocessing steps are applied to prepare the dataset for training. The dataset was properly split into training and test sets, and features were normalized. These steps are crucial for the model's performance and have been executed thoroughly. All the errors are handled adn solved.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Proficiency in Data Preprocessing and Model Development\n",
    "### Data Preprocessing(Data preprocessing is the process of cleaning and transforming raw data into a format suitable for analysis)\n",
    "### Steps:\n",
    "1. Firstly we have checked for any missing values. In our dataset we didn't had any missing values.\n",
    "2. After that we checked for any categorical data in the dataset using info(). Through this we came to know that our dataset contains 1 object i.e., 'letter'. So we dropped that column, then we used LabelEncoder to convert that object into int, and then we stored it in a varibale names as 'y'. (as shown in Task 2)\n",
    "3. We then spil the dataset into training and test sets using train_test_split(). We have given 80% to Train and remaining 20% to test\n",
    "\n",
    "### Model Development:\n",
    "- An ANN model was built with two hidden layers containing 32 and 64 neurons, respectively, and the 'relu' activation function.\n",
    "- Initially we took epoch as 5 which gave a kind of good accuracy of 86%. \n",
    "- To increase the model accuracy we did HyperParameter Tuning and based on the given result we re-run the model and got an accuracy of 95%.\n",
    "\n",
    "\n",
    "## 3. Systematic Approach and Thoroughness in Hyperparameter Tuning\n",
    "### Systematic Approach:\n",
    "- Since we hade a larger datset, we choose RandomizedSearchCV instead of GridSearchCV, as RandomizedSearchCV works well and ffaster for larger datset.\n",
    "- We also used cross-validation(cv) to ensure that the chosen hyperparameters were validated across different subsets of the data. This provides a robust way to identify the optimal model configuration.\n",
    "\n",
    "### Thoroughness in Hyperparameter Tuning:\n",
    "- We have creted a dictionary(parameter grid) as search_dict and passed different hyperparameter like optimizer, neurons,\n",
    "  activation, batch_size, epochs, hidden_layers and learning_rate through it, to get the BEST HP.\n",
    "- After performing Hyperparameter Tuning, we got: \"Best parameters as: {'optimizer': 'adam', 'neurons': 128, 'learning_rate': 0.001, 'hidden_layers': 3, 'epochs': 10, 'batch_size': 20, 'activation': 'relu'}\". Using this we have re-build the model and got accuracy of .\n",
    "\n",
    "\n",
    "## 4. Depth of Evaluation and Discussion\n",
    "- The model's performance was evaluated using accuracy, precision, recall, and F1-score. The high accuracy and other metrics indicate strong performance, with a comprehensive assessment providing confidence in the model’s effectiveness.\n",
    "- Comparison: The performance of the tuned model was compared with the baseline model. The tuned model showed improvements, demonstrating the value of hyperparameter optimization in enhancing model performance.\n",
    "\n",
    "\n",
    "## Analysis of Hyperparameter Tuning Effects:\n",
    "\n",
    "### Accuracy Improvement:\n",
    "- Default Model: 86%\n",
    "\n",
    "- Tuned Model: 95%\n",
    "\n",
    "Conclusion: The accuracy increased by approximately 9% after tuning. This suggests that the tuned model performs significantly better on unseen data, indicating that the tuning process helped in optimizing the model to generalize better.\n",
    "\n",
    "\n",
    "## Loss Reduction:\n",
    "\n",
    "- Default Model: 0.4667\n",
    "\n",
    "- Tuned Model: 0.0113\n",
    "\n",
    "Conclusion: The loss decreased drastically by 0.4554. A lower loss value indicates that the tuned model makes fewer errors and performs better in predicting the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2ad31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b0861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c9a68b9",
   "metadata": {},
   "source": [
    "# Thank You!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2554d3a8",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
